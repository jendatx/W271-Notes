---
title: "ARMA, ARIMA, and SARIMA Models"
author: "Michael Winton"
date: \today
output:
  html_document:
    df_print: paged
  pdf_document: default
header-includes: \usepackage{amsmath}
geometry: margin=1in
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits=4, fig.height=3.5, warn=FALSE)
library(car)
library(dplyr)
library(forecast)
library(stargazer)
```


## ARMA Models

The ARMA model is a stationary model, which means we will need to assume (and verify) that our data are a realization of a stationary process.  Because regression can be used to break down a nonstationary model to a trend, seasonal components, and a residual series, it's often reasonable to apply stationary models to the residuals from a time series regression.

A time series is called a mixed autoregressive moving average model of order $(p,q)$ if it is _stationary_ and takes the form:

$$ x_t =  \phi_1 x_{t-1} + ... + \phi_p x_{t-p}  + w_t + \theta_1 w_{t-1} + ... + \theta_q w_{t-q}$$
We can observe that the model includes a white noise component, an AR component, and a moving average component.  This equation assumes the series has already been demeaned. 

We can rewrite this equation in terms of backshift operators and simplify in order to get a very concise version of the equation.

$$ x_t  - \phi_1 x_{t-1} -  ... - \phi_p x_{t-p} = w_t + \theta_1 w_{t-1} + ... + \theta_q w_{t-q}$$
$$ x_t (1-\phi_1 B - ... - \phi_p B^p) = w_t (1 +  \theta_1 B + ... + \theta_q B^q)$$
$$ x_t (1-\phi_1 B - ... - \phi_p B^p) = w_t (1 +  \theta_1 B + ... + \theta_q B^q)$$

$$ \phi_p(B) x_t = \theta_q(B) w_t$$

Important points about an $ARMA(p,q)$ model:

- the process is _stationary_ when absolute value of all roots of $\phi_p$ > 0
- the process is _invertible_ when absolute value of all roots of $\theta_q$ > 0
- $AR(p)$ model is a special case where $q=0$
- $MA(q)$ model is a special case where $p=0$
- _Parsimony_: an ARMA model will often be more parameter-efficient that either AR or MA by themselves
- Sometimes we can simplify the equation if $\phi_p$ and $\theta_q$ share a common factor

If we wanted to accommodate a non-zero mean $\mu$, then we would add a constant term on the right-hand side:  $\alpha = \mu (1-\phi_1 - ...- \phi_p)$:

$$ x_t = \alpha + w_t +[ \phi_1 x_{t-1} + ... + \phi_p x_{t-p} ] + [\theta_1 w_{t-1} + ... + \theta_q w_{t-q}]$$

The equation for $\alpha$ can also be rearranged to show that the mean $\mu$ is stationary:

$$ \mu = \frac{\alpha}{1-\phi_1 - ...- \phi_p}$$

### Transforming $ARMA(p,q)$ Model to an $MA(\infty)$ or $AR(\infty)$ Model

If we go back to the ARMA equation in terms of the backshift operator:

$$ \phi_p(B) x_t = \theta_q(B) w_t$$

we see that we can rearrange it to get an MA process ($x_t$ as a function of $w_t$):

$$ x_t = \frac {\theta_q(B)}{ \phi_p(B)}  w_t \equiv \psi(B) w_t $$
Through expansion of $\phi_p(B)$ in the denominator as a geometric series, this becomes an $MA(\infty)$ model.

Likewise, we can rearrange to achieve an $AR(\infty)$ model ($w_t$ as a function of $x_t$):

$$ w_t = \frac{ \phi_p(B)} {\theta_q(B)}  x_t \equiv \pi(B) x_t $$

### Derivation of Second Order Properties

_Skipped.  Terminology in async is completely inconsistent from slide to slide.  It seems like it's mainly used to derive an equation corresponding to the form of the ACF plot._

### ACF and PACF plots for AR, MA, and ARMA Models

Reminder about the general properties of the ACF and PACF plots:

Plot | AR(p) Model | MA(q) Model | ARMA (p,q) Model
-----|-------------|-------------|-----------------
ACF  | Tails off   | Abrupt cutoff after lag q  | Tails off
PACF | Abrup.t cutoff after lag p | Tails off   | Tails off

Because the ACF for $ARMA(1,1)$ and $AR(1)$ only differ by a constant, it's hard to use ACF plots to distinguish between the models.  In an $AR(p)$ model, as $p$ approaches 1, the series gets more persistent.  However, even with a smaller $p$, the addition of an $MA(q)$ component will also contribute to persistence.

We may be able to more easily distinguish (in some cases) from a PACF plot.  Recall that the PACF plot for a $AR(p)$ model cuts off abruptly after $p$ lags; a PACF plot for an $ARMA(p,q)$ plot generally will take longer to tail off.

In practice, we will estimate several models and use AIC, BIC, or forecasting a test set to decide between them.

### Example: British Pound - NZ Dollar Exchange Rate

```{r}
df <- read.table('pounds_nz.dat', header=TRUE)
bpnz <- ts(df$xrate)
str(bpnz)
head(bpnz)
tail(bpnz)

par(mfrow=c(2,2))
plot(bpnz)
hist(bpnz)
acf(bpnz)
pacf(bpnz)
```

Observations:

- Series is not stationary, so AR model is not appropriate
- ACF tails off, so MA model doesn't seem a likely fit.  

#### BPNZ - MA(5) Model

We'll try an MA model anyways as a demo:

```{r}
(ma5 <- arima(bpnz, order=c(0,0,5)))  # MA(5) model
AIC(ma5)
BIC(ma5)
```

The first 4 coefficients are statistically significant, but we still need to do diagnostics:

```{r}
ma5r <- resid(ma5)

par(mfrow=c(2,2))
plot(ma5r)
hist(ma5r)
# qqnorm(ma5r)
acf(ma5r)
pacf(ma5r)
```

Observations:

- the residuals do not appear to be a white noise series
- ACF, PACF don't show significant autocorrelations

We also do the Ljung-Box test for autocorrelation of the residual series.  The null hypothesis is of independence (no correlation):

```{r}
Box.test(ma5r, type="Ljung-Box")  # using default of 1 lag
```

The high p-value says we cannot reject the null hypothesis of independence.

Now let's look at in-sample vs. out-of-sample fit:

```{r}
par(mfrow=c(1,1))
ts.plot(bpnz, fitted(ma5), resid(ma5), lwd=c(1,2,1),
        lty=c('solid','dashed','dashed'), col=c('black','blue','black'),
        main='MA(5) model')
```

The in-sample fit looks reasonable.  Here's what an $MA(5)$ model would forecast:

```{r}
# do a forecast of 6 steps
(fcast <- forecast(ma5, h=6))
plot(fcast)
lines(fitted(ma5), lty='dashed', col='blue')
```


Let's try back-testing for out-of-sample performance:
```{r}
bpnz_bt <- window(bpnz, end=33)  # hold out last 5 observations for testing
ma5_bt <- arima(bpnz_bt, order=c(0,0,5))

fcast_bt <- forecast(ma5_bt, h=12)

par(mfrow=c(1,1))
plot(fcast_bt)
lines(window(bpnz, start=34), lty='solid', col='black')
lines(fitted(ma5_bt), lty='dashed', col='blue')
```

Observations:

- the forecast is way off since the actual last 6 observations changed direction
- the forecast stays flat after 5 periods because the $MA(5)$ model can't forecast further out

#### BPNZ - ARMA(1,1) Model

Next we'll try an ARMA model for the sake of demo.  (The requirement of stationarity is not actually met for this time series.)
```{r}
(arma11 <- arima(bpnz, order=c(1,0,1)))  # ARMA(1,1) model
AIC(arma11)
BIC(arma11)

arma11r <- resid(arma11)
par(mfrow=c(2,2))
plot(arma11r)
hist(arma11r)
acf(arma11r)
pacf(arma11r)

Box.test(arma11r, type="Ljung-Box")  # using default of 1 lag

```

Observations:

- AIC and BIC are both better than the MA(5) model
- residual TS plot looks more like white noise
- Ljung-Box test doesn't reject H_0 of uncorrelated residual series
- ACF, PACF doesn't show significant correlations

This looks like a better model for this data.  

Now let's look at in-sample fit and forecast.

```{r}
par(mfrow=c(1,1))
ts.plot(bpnz, fitted(arma11), resid(arma11), lwd=c(1,2,1),
        lty=c('solid','dashed','dashed'), col=c('black','blue','black'),
        main='ARMA(1,1) model')
```

In-sample fit looks reasonable, although generally shifted slightly to the right of actual.

```{r}
(fcast <- forecast(arma11, h=6))
plot(fcast)
lines(fitted(arma11), lty='dashed', col='blue')
```

Observations:

- forecast still trends downwards, but not as fast as the MA(5) model

Now, back-test, holding back 6 observations:

```{r}
arma11_bt <- arima(bpnz_bt, order=c(1,0,1))
fcast_bt <- forecast(arma11_bt, h=12)

par(mfrow=c(1,1))
plot(fcast_bt)
lines(window(bpnz, start=34), lty='solid', col='black')
lines(fitted(arma11_bt), lty='dashed', col='blue')
```

Observations:

- forecast still deviates from actual values
- the 95% confidence interval of the forecast includes most of the actuals

## ARIMA Models

ARIMA models are one way of dealing with _nonstationary_ time series, which may have trends or seasonal effects.  Simple "differencing" can off convert a _nonstationary_ time series to a _stationary_ one.  Often a first-order differencing is sufficient (but generally we don't want to go higher than 2nd order).

Note: not all nonstationary time series can be deal with by differencing.  Notably, volatility clustering (conditional heteroskedasticity) that's common in financial times series requires a different kind of model.  Those are commonly model with Autoregressive Conditional Heteroskedastic (ARCH) models. 

The term "integrated" arises from the fact that a differenced series needs to be aggregated in order to recover the original.  The simplest $I(0)$ process is white noise; the simplest $I(1)$ process is the random walk (because after first differencing, we have a white noise model).


### First Differencing of a Random Walk

For example, a random walk has the following form:

$$ x_t = x_{t-1} + w_t$$

Rearranged, we get a stationary white noise series $w_t \sim N(0,\sigma_w^2)$:

$$ x_t - x_{t-1} \equiv \nabla x_t = w_t $$

Just a quick refresher as to random walk time series look like.  Note that the drift term plays the same role as the slope in deterministic linear trend models.

```{r}
par(mfrow=c(2,1))

# without drift
x <- w <- rnorm(100)
for (t in 2:length(x)) {
  x[t] <- x[t-1] + w[t]
}
plot(ts(x), main='Random Walk Simulation')

# with drift
x <- w <- rnorm(100)
del <- 0.5
for (t in 2:length(x)) {
  x[t] <- del + x[t-1] + w[t]
}
plot(ts(x), main='Random Walk Simulation w/ Drift (delta=0.5)')
```

As a reminder, the expectation value grows over time due to the drift:

$$E(x_t)=x_0 + t \delta$$

and the variance grows without bounds:
  
$$ Var(x_t) = t \sigma^2$$
  
The autocovariance simplifies to: 
  
$$\gamma_k = t \sigma^2$$
  
Because autocovariance is a function of time, this model is obviously _nonstationary_.  

### First Differencing of a Linear Trend

First differencing can also remove deterministic trends.  If we have linear trend of the following form, we can consider either first differencing (which results in an $MA(1)$ progress) or simply subtracting the trend (and analyzing residuals):

$$ x_t = a + bt + w_t$$
$$ \nabla x_t = x_t - x_{t-1} = (a - a) + (bt - b(t-1)) + (w_t - w_{t-1}) = b + w_t - w_{t-1}$$
Reducing and rewriting in terms of the lag operator, we have a stationary $MA(1)$ process:

$$ \nabla x_t  = b + \theta_q(B) w_t$$
Alternately, we could have subtracted the trend (instead of differencing) to achieve a white noise process:

$$ x_t - (a + bt) = a + bt + w_t - (a + bt) = w_t$$
If our original time series showed increasing _variance_ over time, we should try a log transformation (and then differencing, if there's also a trend we want to try to remove).

### ARIMA Terminology

An $ARIMA(p,d,q)$ model is an $ARMA(p,q)$ model that's applied after taking the $d^{th}$ difference of the original time series $x_t$.  Using the lag operator, this can be expressed as:

$$ \phi_p(B)(1-B)^d x_t = \theta_q(B) w_t$$

### Simulate an ARIMA time series and estimate it

Let's simulate this model:

$$ x_t = 0.5 x_{t-1} + x_{t-1} - 0.5 x_{t-2} + w_t +0.3 w_{t-1}$$
Rearranging:
$$ x_t - x_{t-1} = 0.5(x_{t-1} - x_{t-2}) + w_t + 0.3 w_{t-1}$$
$$ x_t - x_{t-1} - 0.5(x_{t-1} - x_{t-2}) = w_t + 0.3 w_{t-1}$$
$$ \nabla x_t - 0.5 \nabla x_{t-1} = w_t - w_{t-1}$$
Since we have $x_t$ and $x_{t-1}$ terms, we can rewrite in terms of the lag operator.  This has the form of an $ARIMA(1,1,1)$ model:

$$ \nabla x_t (1 - 0.5B) = (1 + 0.3B)w_t$$
$$ \nabla x_t = 0.5B \nabla x_t + (1+0.3B)w_t$$
$$ \nabla x_t - 0.5 \nabla x_{t-1} = w_t + 0.3 w_{t-1} $$
We see that after applying the first differencing operator $\nabla$, the $ARIMA(1,1,1)$ model is transformed to an $ARMA(1,1)$ stationary model with an AR parameter $\phi_1 = 0.5$ and an MA parameter of $\theta_q=0.3$.  (_Be careful of signs!_)

Let's simulate this equation, and then compare plots between the original and the differenced equation:

```{r}
x <- w <- rnorm(100)
for (t in 3:100) {
  x[t] = 0.5 * x[t-1] + x[t-1] - 0.5 * x[t-2] + w[t] + 0.3 * w[t-1]
}
x_ts <- ts(x)

# alternately, we could have asked arima.sim to run the simulation 
# x_ts <- arima.sim(model=list(order=c(1,1,1), ar=0.5, ma=0.3), n=100)

par(mfrow=c(1,2))
plot(x_ts, main='Original Simulated Time Series')
plot(diff(x_ts), main='Differenced Simulated Time Series')

acf(x_ts, main='Original Simulated Time Series')
acf(diff(x_ts), main='Differenced Simulated Time Series')

pacf(x_ts, main='Original Simulated Time Series')
pacf(diff(x_ts), main='Differenced Simulated Time Series')
```

Observations:

- Differenced TS appears more stationary on the TS plot
- ACF of differenced TS dies off much more quickly; suggests MA component
- PACF of original and differenced TS die off after 1 lag; maybe AR(1)?

Now let's estimate the model and plot the original and fitted time series.
```{r}
(arima111 <- arima(x_ts, order=c(1,1,1)))
(arma11_diffed <- arima(diff(x_ts), order=c(1,0,1)))

par(mfrow=c(1,1))
ts.plot(x_ts, fitted(arima111), lwd=c(1,2),
        lty=c('solid','dashed'), col=c('black','blue'),
        main='ARIMA(1,1,1) model')

```

We see essentially the same results when we manually difference before fitting an ARMA model.  Our estimated coefficients are quite close to the true (supposedly unknown) process that we simulated.  Our fitted values from the model are quite close to the originals.

Now let's do our standard diagnostics with the model's residuals:

```{r}
arima111r <- resid(arima111)
par(mfrow=c(2,2))
plot(arima111r, main='Residual TS')
hist(arima111r)
acf(arima111r)
pacf(arima111r)

Box.test(arima111r, type='Ljung-Box')
```

The plots and Ljung-Box test both fail to reject independence (ie. supporting the residual TS as a realization of a white noise process), indicating that we should be able to forecast with this model.

Let's attempt forecasting:
```{r}
(fcast <- forecast(arima111, h=12))
par(mfrow=c(1,1))
plot(fcast)
lines(fitted(arima111), lty='dashed', col='blue')
```

We observe that while the model fit is good, the forecast is essentially a flat line. 

## Seasonal ARIMA Models (SARIMA)

These are basically an extension of ARIMA models that add a lag equal to a number of seasons in order to remove seasonal effects.  The form of the model is $ARIMA(p,d,q)(P,D,Q)_m$ where $p,d,q$ are the nonseasonal lag terms, and $P,D,Q$ are the seasonal lag terms, where $m$ is the number of periods per year (e.g. $m=12$ for monthly seasonality, or $m=4$ for quarterly seasonality).  The $P,D,Q$ terms basically just represent backshifts of a seasonal period.  We use lowercase $\phi_p,\theta_q$ to represent nonseasonal components, and $\Phi_P, \Theta_Q$ to represent the seasonal terms:

$$ \Phi_P(B^m)  (1-B^m)^D \phi_p(B) (1-B)^d x_t = \Theta_Q (B^m) \theta_q(B) w_t$$
These terms correspond to:

$$ [\text{Seasonal AR(P)}][\text{Seasonal Diff}][\text{Non-seasonal AR(p)}][\text{Non-seasonal Diff}] = [\text{Seasonal MA(Q)}][\text{Non-seasonal MA(q)}]$$

For, example a monthly $ARIMA(1,1,1)(1,1,1)_4$ model looks like:

$$ (1-\Phi_1 B^4) (1-B^4)^1 (1-\phi_1 B) (1-B)x_t = (1+ \Theta_1B^4)(1+\theta_1B)w_t$$
### SARIMA Examples

#### EXAMPLE 1: $ARIMA(0,0,0)(0,0,1)_12$

This model indicates that a monthly value is affected by the same month's value in the previous year, is:

$$x_t = \alpha x_{t-12} + w_t$$ 

The characteristic equation for that formula is:

$$ (1 - \alpha B^{12}) = 0$$
Rearrange, we get:

$$ B = \frac{1}{\alpha}^{1/12} = \alpha^{-1/12}$$
The model is stationary when $|\alpha^{-1/12}|>1$.

#### EXAMPLE 2: $ARIMA(0,1,0)(1,0,0)_12$

$$ x_t = x_{t-1} + \alpha x_{t-12} - \alpha x_{t-13} + w_t$$
We could also write as:
$$ \nabla x_t = \alpha \nabla x_{t-12} + w_t$$
which helps us understand the intuition that the _change_ at time $ t $ depends on the _change_ at the same time of the previous year. Rearranging and factorizing gives:

$$ \Theta_1(B^{12})(1-B)x_t = (1-\alpha B^{12})(1-B)x_t = w_t$$
We see the seasonal AR(1) term, with $m=12$, and the nonseasonal difference term.  This model is nonstationary, since we see that there is a root of $B=1$.

#### EXAMPLE 3: Variants of seasonal moving average models

A quarterly, seasonal moving average model $ARIMA(0,0,0)(0,0,1)_4$ (stationary, without a trend) is:

$$ x_t = (1-\beta B^4)w_t = w_t - \beta w_{t-4}$$
If a nonseasonal stochastic trend was also present, a $ARIMA(0,1,0)(0,0,1)_4$ model (using 1st order difference to remove the trend) could look like:

$$ x_t = x_{t-1} + w_t - \beta w_{t-4}$$
If, instead, a seasonal stochastic trend was present, the $ARIMA(0,0,0)(0,1,1)_4$ model could be used to remove the seasonal stochastic trend:

$$ x_t = x_{t-4} + w_t - \beta w_{t-4}$$
### Worked example: EU Retail Trade Index (Quarterly)

```{r}
library(fpp) # samples for Rob Hyndman's book
data(euretail)
str(euretail)
```

Plot the time series:
```{r}
par(mfrow=c(3,1))
# original time series
plot(euretail)
# take seasonal difference (still not stationary)
plot(diff(euretail, lag=4))
# take nonseasonal and seasonal differences (better)
plot(diff(diff(euretail, lag=4)))
```

Now let's try modeling it.  Note that if we use `sarima(...)`, then we need a slightly different form to retrieve the residuals.  The function comes with a nice, default display.

```{r}
library(astsa)
eu_arima <- sarima(euretail, 0,1,1,0,1,1,4)
eu_arima$ttable  # display estimated parameters
eu_arima$AICc  # AIC, BIC are also available
par(mfrow=c(2,2))
eu_arimar <- resid(eu_arima$fit)
plot(eu_arimar)
hist(eu_arimar)
acf(eu_arimar)
pacf(eu_arimar)
```

Since both the ACF and PACF show significant spikes at lag $k=2$, this suggests we need to include additional nonseasonal terms.  (We don't see signifiant seasonal lags.)  Also observe that the Ljung-Box plot (nice!) rejects the null hypthesis of independence of the residuals.  So, we need to do more work on a better model.  Next, I'll go straight to the best model that the book found:

```{r}
#eu_arima <- sarima(euretail, 0,1,3,0,1,1,4)
# eu_arima$ttable  # display estimated parameters
# eu_arima$AICc  # AIC, BIC are also available
# eu_arimar <- resid(eu_arima$fit)

(eu_arima <- arima(euretail, order=c(0,1,3), seasonal=list(order=c(0,1,1), period=4)))
par(mfrow=c(2,2))
eu_arimar <- resid(eu_arima)
plot(eu_arimar)
hist(eu_arimar)
acf(eu_arimar)
pacf(eu_arimar)
Box.test(eu_arimar, type='Ljung-Box')
```

**NOTE: I used `arima` instead of `sarima` because the latter produced an object that I couldn't pass to `forecast`.**

Observations:

- Residaul TS, ACF, PACF appear representative of white noise (no significant lags)
- AICc is better than for the earlier model
- Ljung-Box test fails to reject the null hypothesis of independence of residuals (good)

Now, let's try a forecast:

```{r}
(fcast <- forecast(eu_arima, h=12))
par(mfrow=c(1,1))
plot(fcast)
lines(fitted(eu_arima), lty='dashed', col='blue')
```

Observations:

- the forecast follows the most recent trend
- due to large CI, neither increasing nor decreasing trends can be ruled out

### Worked example: AUS Electricity Production

First load and take a quick look at the dataset.

```{r}
cbe <- read.table('cbe.dat', header=TRUE)
head(cbe)
elec <- ts(cbe$elec, start=1958, freq=12)
str(elec)
head(elec)
tail(elec)
summary(elec)
quantile(elec, c(0.01,0.05,0.1,0.25,0.5,0.75,0.9,0.95,0.99))
```
Now look at the plots.
```{r}
par(mfrow=c(2,2))
plot(elec)
hist(elec)
acf(elec)
pacf(elec)
```

The first difference of removes the trend, but not the variance.  First difference of log transformed time series removes both:

```{r}
par(mfrow=c(3,1))
plot(elec)
plot(diff(elec), main='First Difference of TS')
plot(diff(log(elec)), main='First Difference of Log(TS)')
```
We can see the improvement quantitatively:

```{r}
summary(elec)
summary(diff(elec))
summary(diff(log(elec)))
```
Now look at the ACF and PACF of the differenced and log-transformed dataset:

```{r}
elec_trans <- diff(log(elec))
par(mfrow=c(1,2))
acf(elec_trans, lag.max=48)
pacf(elec_trans, lag.max=48)
```

Both still show seasonal effects, made even more clear by extending the lag axis.  I can't duplicate the results in the async slides, so I'll instead replicate the best model found in the book.  (See Cowper page 144 for an example of a function to determine the best parameters.)

```{r}
(elec_arima <- arima(log(elec), order=c(0,1,1),
                     seasonal=list(order=c(2,0,2), period=frequency(log(elec)))))

elec_arimar <- resid(elec_arima)
par(mfrow=c(2,2))
plot(elec_arimar)
hist(elec_arimar)
acf(elec_arimar)
pacf(elec_arimar)

Box.test(elec_arimar, type='Ljung-Box')
```
Observations:

- Residaul TS, ACF, PACF appear representative of white noise (no significant lags)
- Ljung-Box test fails to reject the null hypothesis of independence of residuals (good)

Now, try forecasting:

```{r}
(fcast <- forecast(elec_arima, h=12))
par(mfrow=c(1,1))
plot(fcast)
lines(fitted(elec_arima), lty='dashed', col='blue')
```

Observations:

- the fitted points very closely follow the actual observations
- finally we see a time series where the CI on the forecast is quite tight!


We need to manually exponentiate fitted and forecasted values if we them on original scale:

```{r}
# exponentiate the forecasted values
exp_fitted <- exp(fcast$fitted)
exp_fcast <- exp(fcast$mean)
exp_upper95 <- exp(fcast$upper[,'95%'])
exp_lower95 <- exp(fcast$lower[,'95%'])
exp_upper80 <- exp(fcast$upper[,'80%'])
exp_lower80 <- exp(fcast$lower[,'80%'])

# ts.plot(elec, exp_fcast, exp_fitted, exp_lower95, exp_upper95, lwd=c(1,2,1,1,1),
#         lty=c('solid','dashed', 'dashed','solid','solid'), col=c('black','blue','blue','gray','gray'),
#         main='Exponentiated Forecast (Original Units)', xlim=c(1980,1992))

# manually recreate the forecast plot (with CI) using my exponentiated series
plot(elec, xlim=c(1980,1992), ylim=c(6000,16000), main='Exponentiated Forecast (Original Units)')
tmp <- seq(from = 1991, by = 1/12, length = 12)  # need x axis for drawing polygon
polygon(x= c(tmp,rev(tmp)),y= c(exp_upper95,rev(exp_lower95)), col="lightgray", border=NA)
polygon(x= c(tmp,rev(tmp)),y= c(exp_upper80,rev(exp_lower80)), col="lightsteelblue3", border=NA)
lines(exp_fcast, lwd=2, lty='solid', col='blue')
# lines(exp_upper95, lwd=1, lty='dotted', col='blue')
# lines(exp_lower95, lwd=1, lty='dotted', col='blue')
```

## Review: General Steps in Time Series Analysis

1. Based on theory, subject matter knowledge, experience - choose a useful class of models
2. Cleanse the data
3. Conduct EDA: plot and examine main patterns, atypical observations:
    - Trend
    - Fluctuation around a trend (seasonal or cyclical/non-seasonal)
    - Sharp changes in behavior
    - Outliers
    - See if ACF, PACF plots suggest we should try $AR(p)$ or $MA(q)$ models
4. Examine and statistically test for stationarity
5. If not stationary (and model requires it), transform it:
    - Detrend
    - Remove seasonality
    - Log transformation (especially if variance increases with time)
    - Take 1-2 differences
6. Model TS with stationary or integrated TS model
7. Examine validity of model's underlying assumptions:
    - Plot residual time series, histogram, ACF, and PACF (try to confirm resids look like white noise)
    - Ljung-Box test of the residual time series (failing to reject H_0 is good)
8. Among valid models, pick the one with best AIC, AICc, BIC, test set, etc... performance
9. Conduct inference and/or forecasting (only if model assumptions were validated!)
  
