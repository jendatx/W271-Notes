---
title: "ARMA, ARIMA, and SARIMA Models"
author: "Michael Winton"
date: \today
output:
  html_document:
    df_print: paged
  pdf_document: default
header-includes: \usepackage{amsmath}
geometry: margin=1in
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits=4, fig.height=3.5, warn=FALSE)
library(car)
library(dplyr)
library(forecast)
library(stargazer)
```


## ARMA Models

The ARMA model is a stationary model, which means we will need to assume (and verify) that our data are a realization of a stationary process.  Because regression can be used to break down a nonstationary model to a trend, seasonal components, and a residual series, it's often reasonable to apply stationary models to the residuals from a time series regression.

A time series is called a mixed autoregressive moving average model of order $(p,q)$ if it is _stationary_ and takes the form:

$$ x_t =  \phi_1 x_{t-1} + ... + \phi_p x_{t-p}  + w_t + \theta_1 w_{t-1} + ... + \theta_q w_{t-q}$$
We can observe that the model includes a white noise component, an AR component, and a moving average component.  This equation assumes the series has already been demeaned. 

We can rewrite this equation in terms of backshift operators and simplify in order to get a very concise version of the equation.

$$ x_t  - \phi_1 x_{t-1} -  ... - \phi_p x_{t-p} = w_t + \theta_1 w_{t-1} + ... + \theta_q w_{t-q}$$
$$ x_t (1-\phi_1 B - ... - \phi_p B^p) = w_t (1 +  \theta_1 B + ... + \theta_q B^q)$$
$$ x_t (1-\phi_1 B - ... - \phi_p B^p) = w_t (1 +  \theta_1 B + ... + \theta_q B^q)$$

$$ \phi_p(B) x_t = \theta_q(B) w_t$$

Important points about an $ARMA(p,q)$ model:

- the process is _stationary_ when absolute value of all roots of $\phi_p$ > 0
- the process is _invertible_ when absolute value of all roots of $\theta_q$ > 0
- $AR(p)$ model is a special case where $q=0$
- $MA(q)$ model is a special case where $p=0$
- _Parsimony_: an ARMA model will often be more parameter-efficient that either AR or MA by themselves
- Sometimes we can simplify the equation if $\phi_p$ and $\theta_q$ share a common factor

If we wanted to accommodate a non-zero mean $\mu$, then we would add a constant term on the right-hand side:  $\alpha = \mu (1-\phi_1 - ...- \phi_p)$:

$$ x_t = \alpha + w_t +[ \phi_1 x_{t-1} + ... + \phi_p x_{t-p} ] + [\theta_1 w_{t-1} + ... + \theta_q w_{t-q}]$$

The equation for $\alpha$ can also be rearranged to show that the mean $\mu$ is stationary:

$$ \mu = \frac{\alpha}{1-\phi_1 - ...- \phi_p}$$

### Transforming $ARMA(p,q)$ Model to an $MA(\infty)$ or $AR(\infty)$ Model

If we go back to the ARMA equation in terms of the backshift operator:

$$ \phi_p(B) x_t = \theta_q(B) w_t$$

we see that we can rearrange it to get an MA process ($x_t$ as a function of $w_t$):

$$ x_t = \frac {\theta_q(B)}{ \phi_p(B)}  w_t \equiv \psi(B) w_t $$
Through expansion of $\phi_p(B)$ in the denominator as a geometric series, this becomes an $MA(\infty)$ model.

Likewise, we can rearrange to achieve an $AR(\infty)$ model ($w_t$ as a function of $x_t$):

$$ w_t = \frac{ \phi_p(B)} {\theta_q(B)}  x_t \equiv \pi(B) x_t $$

### Derivation of Second Order Properties

_Skipped.  Terminology in async is completely inconsistent from slide to slide.  It seems like it's mainly used to derive an equation corresponding to the form of the ACF plot._

### ACF and PACF plots for AR, MA, and ARMA Models

Reminder about the general properties of the ACF and PACF plots:

Plot | AR(p) Model | MA(q) Model | ARMA (p,q) Model
-----|-------------|-------------|-----------------
ACF  | Tails off   | Abrupt cutoff after lag q  | Tails off
PACF | Abrup.t cutoff after lag p | Tails off   | Tails off

Because the ACF for $ARMA(1,1)$ and $AR(1)$ only differ by a constant, it's hard to use ACF plots to distinguish between the models.  In an $AR(p)$ model, as $p$ approaches 1, the series gets more persistent.  However, even with a smaller $p$, the addition of an $MA(q)$ component will also contribute to persistence.

We may be able to more easily distinguish (in some cases) from a PACF plot.  Recall that the PACF plot for a $AR(p)$ model cuts off abruptly after $p$ lags; a PACF plot for an $ARMA(p,q)$ plot generally will take longer to tail off.

In practice, we will estimate several models and use AIC, BIC, or forecasting a test set to decide between them.

### Example: British Pound - NZ Dollar Exchange Rate

```{r}
df <- read.table('pounds_nz.dat', header=TRUE)
bpnz <- ts(df$xrate)
str(bpnz)
head(bpnz)
tail(bpnz)

par(mfrow=c(2,2))
plot(bpnz)
hist(bpnz)
acf(bpnz)
pacf(bpnz)
```

Observations:

- Series is not stationary, so AR model is not appropriate
- ACF tails off, so MA model doesn't seem a likely fit.  

#### BPNZ - MA(5) Model

We'll try an MA model anyways as a demo:

```{r}
(ma5 <- arima(bpnz, order=c(0,0,5)))  # MA(5) model
AIC(ma5)
BIC(ma5)
```

The first 4 coefficients are statistically significant, but we still need to do diagnostics:

```{r}
ma5r <- resid(ma5)

par(mfrow=c(2,2))
plot(ma5r)
hist(ma5r)
# qqnorm(ma5r)
acf(ma5r)
pacf(ma5r)
```

Observations:

- the residuals do not appear to be a white noise series
- ACF, PACF don't show significant autocorrelations

We also do the Ljung-Box test for autocorrelation of the residual series.  The null hypothesis is of independence (no correlation):

```{r}
Box.test(ma5r, type="Ljung-Box")  # using default of 1 lag
```

The high p-value says we cannot reject the null hypothesis of independence.

Now let's look at in-sample vs. out-of-sample fit:

```{r}
par(mfrow=c(1,1))
ts.plot(bpnz, fitted(ma5), resid(ma5), lwd=c(1,2,1),
        lty=c('solid','dashed','dashed'), col=c('black','blue','black'),
        main='MA(5) model')
```

The in-sample fit looks reasonable.  Here's what an $MA(5)$ model would forecast:

```{r}
# do a forecast of 6 steps
(fcast <- forecast(ma5, h=6))
plot(fcast)
lines(fitted(ma5), lty='dashed', col='blue')
```


Let's try back-testing for out-of-sample performance:
```{r}
bpnz_bt <- window(bpnz, end=33)  # hold out last 5 observations for testing
ma5_bt <- arima(bpnz_bt, order=c(0,0,5))

fcast_bt <- forecast(ma5_bt, h=12)

par(mfrow=c(1,1))
plot(fcast_bt)
lines(window(bpnz, start=34), lty='solid', col='black')
lines(fitted(ma5_bt), lty='dashed', col='blue')
```

Observations:

- the forecast is way off since the actual last 6 observations changed direction
- the forecast stays flat after 5 periods because the $MA(5)$ model can't forecast further out

#### BPNZ - ARMA(1,1) Model

Next we'll try an ARMA model for the sake of demo.  (The requirement of stationarity is not actually met for this time series.)
```{r}
(arma11 <- arima(bpnz, order=c(1,0,1)))  # ARMA(1,1) model
AIC(arma11)
BIC(arma11)

arma11r <- resid(arma11)
par(mfrow=c(2,2))
plot(arma11r)
hist(arma11r)
acf(arma11r)
pacf(arma11r)

Box.test(arma11r, type="Ljung-Box")  # using default of 1 lag

```

Observations:

- AIC and BIC are both better than the MA(5) model
- residual TS plot looks more like white noise
- Ljung-Box test doesn't reject H_0 of uncorrelated residual series
- ACF, PACF doesn't show significant correlations

This looks like a better model for this data.  

Now let's look at in-sample fit and forecast.

```{r}
par(mfrow=c(1,1))
ts.plot(bpnz, fitted(arma11), resid(arma11), lwd=c(1,2,1),
        lty=c('solid','dashed','dashed'), col=c('black','blue','black'),
        main='ARMA(1,1) model')
```

In-sample fit looks reasonable, although generally shifted slightly to the right of actual.

```{r}
(fcast <- forecast(arma11, h=6))
plot(fcast)
lines(fitted(arma11), lty='dashed', col='blue')
```

Observations:

- forecast still trends downwards, but not as fast as the MA(5) model

Now, back-test, holding back 6 observations:

```{r}
arma11_bt <- arima(bpnz_bt, order=c(1,0,1))
fcast_bt <- forecast(arma11_bt, h=12)

par(mfrow=c(1,1))
plot(fcast_bt)
lines(window(bpnz, start=34), lty='solid', col='black')
lines(fitted(arma11_bt), lty='dashed', col='blue')
```

Observations:

- forecast still deviates from actual values
- the 95% confidence interval of the forecast includes most of the actuals

## ARIMA Models

ARIMA models are one way of dealing with _nonstationary_ time series, which may have trends or seasonal effects.  Simple "differencing" can off convert a _nonstationary_ time series to a _stationary_ one.  Often a first-order differencing is sufficient (but generally we don't want to go higher than 2nd order).

Note: not all nonstationary time series can be deal with by differencing.  Notably, volatility clustering (conditional heteroskedasticity) that's common in financial times series requires a different kind of model.  Those are commonly model with Autoregressive Conditional Heteroskedastic (ARCH) models. 

The term "integrated" arises from the fact that a differenced series needs to be aggregated in order to recover the original.  The simplest $I(0)$ process is white noise; the simplest $I(1)$ process is the random walk (because after first differencing, we have a white noise model).


### First Differencing of a Random Walk

For example, a random walk has the following form:

$$ x_t = x_{t-1} + w_t$$

Rearranged, we get a stationary white noise series $w_t \sim N(0,\sigma_w^2)$:

$$ x_t - x_{t-1} \equiv \nabla x_t = w_t $$

Just a quick refresher as to random walk time series look like.  Note that the drift term plays the same role as the slope in deterministic linear trend models.

```{r}
par(mfrow=c(2,1))

# without drift
x <- w <- rnorm(100)
for (t in 2:length(x)) {
  x[t] <- x[t-1] + w[t]
}
plot(ts(x), main='Random Walk Simulation')

# with drift
x <- w <- rnorm(100)
del <- 0.5
for (t in 2:length(x)) {
  x[t] <- del + x[t-1] + w[t]
}
plot(ts(x), main='Random Walk Simulation w/ Drift (delta=0.5)')
```

As a reminder, the expectation value grows over time due to the drift:

$$E(x_t)=x_0 + t \delta$$

and the variance grows without bounds:
  
$$ Var(x_t) = t \sigma^2$$
  
The autocovariance simplifies to: 
  
$$\gamma_k = t \sigma^2$$
  
Because autocovariance is a function of time, this model is obviously _nonstationary_.  

### First Differencing of a Linear Trend

First differencing can also remove deterministic trends.  If we have linear trend of the following form, we can consider either first differencing (which results in an $MA(1)$ progress) or simply subtracting the trend (and analyzing residuals):

$$ x_t = a + bt + w_t$$
$$ \nabla x_t = x_t - x_{t-1} = (a - a) + (bt - b(t-1)) + (w_t - w_{t-1}) = b + w_t - w_{t-1}$$
Reducing and rewriting in terms of the lag operator, we have a stationary $MA(1)$ process:

$$ \nabla x_t  = b + \theta_q(B) w_t$$
Alternately, we could have subtracted the trend (instead of differencing) to achieve a white noise process:

$$ x_t - (a + bt) = a + bt + w_t - (a + bt) = w_t$$
If our original time series showed increasing _variance_ over time, we should try a log transformation (and then differencing, if there's also a trend we want to try to remove).

### ARIMA Terminology

An $ARIMA(p,d,q)$ model is an $ARMA(p,q)$ model that's applied after taking the $d^{th}$ difference of the original time series $x_t$.  Using the lag operator, this can be expressed as:

$$ \phi_p(B)(1-B)^d x_t = \theta_q(B) w_t$$

### Simulate an ARIMA time series and estimate it

Let's simulate this model:

$$ x_t = 0.5 x_{t-1} + x_{t-1} - 0.5 x_{t-2} + w_t +0.3 w_{t-1}$$
Rearranging:
$$ x_t - x_{t-1} = 0.5(x_{t-1} - x_{t-2}) + w_t + 0.3 w_{t-1}$$
$$ x_t - x_{t-1} - 0.5(x_{t-1} - x_{t-2}) = w_t + 0.3 w_{t-1}$$
$$ \nabla x_t - 0.5 \nabla x_{t-1} = w_t - w_{t-1}$$
Since we have $x_t$ and $x_{t-1}$ terms, we can rewrite in terms of the lag operator.  This has the form of an $ARIMA(1,1,1)$ model:

$$ \nabla x_t (1 - 0.5B) = (1 + 0.3B)w_t$$
$$ \nabla x_t = 0.5B \nabla x_t + (1+0.3B)w_t$$
$$ \nabla x_t - 0.5 \nabla x_{t-1} = w_t + 0.3 w_{t-1} $$
We see that after applying the first differencing operator $\nabla$, the $ARIMA(1,1,1)$ model is transformed to an $ARMA(1,1)$ stationary model with an AR parameter $\phi_1 = 0.5$ and an MA parameter of $\theta_q=0.3$.  (_Be careful of signs!_)

Let's simulate this equation, and then compare plots between the original and the differenced equation:

```{r}
x <- w <- rnorm(100)
for (t in 3:100) {
  x[t] = 0.5 * x[t-1] + x[t-1] - 0.5 * x[t-2] + w[t] + 0.3 * w[t-1]
}
x_ts <- ts(x)

# alternately, we could have asked arima.sim to run the simulation 
# x_ts <- arima.sim(model=list(order=c(1,1,1), ar=0.5, ma=0.3), n=100)

par(mfrow=c(1,2))
plot(x_ts, main='Original Simulated Time Series')
plot(diff(x_ts), main='Differenced Simulated Time Series')

acf(x_ts, main='Original Simulated Time Series')
acf(diff(x_ts), main='Differenced Simulated Time Series')

pacf(x_ts, main='Original Simulated Time Series')
pacf(diff(x_ts), main='Differenced Simulated Time Series')
```

Observations:

- Differenced TS appears more stationary on the TS plot
- ACF of differenced TS dies off much more quickly; suggests MA component
- PACF of original and differenced TS die off after 1 lag; maybe AR(1)?

Now let's estimate the model and plot the original and fitted time series.
```{r}
(arima111 <- arima(x_ts, order=c(1,1,1)))
(arma11_diffed <- arima(diff(x_ts), order=c(1,0,1)))

par(mfrow=c(1,1))
ts.plot(x_ts, fitted(arima111), lwd=c(1,2),
        lty=c('solid','dashed'), col=c('black','blue'),
        main='ARIMA(1,1,1) model')

```

We see essentially the same results when we manually difference before fitting an ARMA model.  Our estimated coefficients are quite close to the true (supposedly unknown) process that we simulated.  Our fitted values from the model are quite close to the originals.

Now let's do our standard diagnostics with the model's residuals:

```{r}
arima111r <- resid(arima111)
par(mfrow=c(2,2))
plot(arima111r, main='Residual TS')
hist(arima111r)
acf(arima111r)
pacf(arima111r)

Box.test(arima111r, type='Ljung-Box')
```

The plots and Ljung-Box test both fail to reject independence (ie. supporting the residual TS as a realization of a white noise process), indicating that we should be able to forecast with this model.

Let's attempt forecasting:
```{r}
(fcast <- forecast(arima111, h=12))
par(mfrow=c(1,1))
plot(fcast)
lines(fitted(arima111), lty='dashed', col='blue')
```

We observe that while the model fit is good, the forecast is essentially a flat line. WHY???

## Review: General Steps in Time Series Analysis

1. Based on theory, subject matter knowledge, experience - choose a useful class of models
2. Cleanse the data
3. Conduct EDA: plot and examine main patterns, atypical observations:
    - Trend
    - Fluctuation around a trend (seasonal or cyclical/non-seasonal)
    - Sharp changes in behavior
    - Outliers
    - See if ACF, PACF plots suggest we should try $AR(p)$ or $MA(q)$ models
4. Examine and statistically test for stationarity
5. If not stationary (and model requires it), transform it:
    - Detrend
    - Remove seasonality
    - Log transformation (especially if variance increases with time)
    - Take 1-2 differences
6. Model TS with stationary or integrated TS model
7. Examine validity of model's underlying assumptions:
    - Plot residual time series, histogram, ACF, and PACF (try to confirm resids look like white noise)
    - Ljung-Box test of the residual time series (failing to reject H_0 is good)
8. Among valid models, pick the one with best AIC, AICc, BIC, test set, etc... performance
9. Conduct inference and/or forecasting (only if model assumptions were validated!)
  
