---
title: "Time Series Autoregressive Models"
author: "Michael Winton"
date: \today
output:
  html_document:
    df_print: paged
  pdf_document: default
header-includes: \usepackage{amsmath}
geometry: margin=1in
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits=4, fig.height=3.5, warn=FALSE)
library(car)
library(dplyr)
library(forecast)
library(stargazer)
```

## Backshift Operator

The AR model is written as:
$$ x_t =  \alpha_1 x_{t-1} + \alpha_2 x_{t-2} + ... + \alpha_p x_{t-p} + w_t$$

With the backward shift operator, we can rewrite this as:
$$ x_t = \alpha_1 B + \alpha_2 B^2 + ... \alpha_p B^p + w_t$$
Rearrange to get $w_t$ by itself:
$$ (1 - \alpha_1 B - \alpha_2 B^2 - ... \alpha_p B^p) x_t = w_t$$

We define $\theta_p(B)$ as this coefficient of $x_t$:
$$ \theta_p(B) =  (1 - \alpha_1 B - \alpha_2 B^2 - ... \alpha_p B^p) $$
which gives:
$$ \theta_p(B) x_t =w_t$$

The _characteristic equation_ is defined by $\alpha(B) = 0$:

$$ \theta_p(B) =  (1 - \alpha_1 B - \alpha_2 B^2 - ... \alpha_p B^p) = 0 $$

This is an important tool to check if a process is stationary.  **If _all_ roots for $B$ have an absolute value > 1, then process $x_t$ is stationary.**  This basically means they all sit outside a unit circle.


### Alternate terminologies

In the lectures, we sometimes see these alternate terminologies:

- $\phi(B)$ or $\Phi(B)$ are used in some places instead of $\theta_p(B)$
- $L$ (for lag) instead of $B$ (for backshift)
- $\epsilon_t$ (error) instead of $w_t$ (white noise)

## Key Properties of a General AR(p) model

1. Stationarity condition - an AR(p) process is covariance stationary if and only if all roots of the autoregressive lag operator polynomial $\alpha(B)$ are outside the unit circle (also meaning that the _inverse_ of these roots is inside the unit circle).

2. ACF - the autocorrelation function of the AR(p) process decays gradually, with displacement.

3. PACF - the partial autocorrelation function of the AR(p) process has a sharp cutoff at displacement p.

Models with higher autoregressive order can have richer dynamics, with the ACF displaying a wider variety of patterns (e.g. it can have damped oscilliation that an AR(1) model could only have with a negative coefficient).  These richer patterns can mimic a wider range of _cyclical_ patterns.

## Example AR(2) simulation

We'll run a simulation with the model:

$$ x_t = 1.5 x_{t-1} - 0.9 x_{t-2} + w_t $$

Rewritten in terms of the backshift operator:
$$\theta_p(B) = (1 - 1.5B + 0.9B^2)x_t = w_t$$
Since we can't easily factor this manually, use `polyroot`:

```{r}
(roots <- polyroot(c(1, -1.5, 0.9)))
abs(roots)

(inverse_roots <- 1/roots)
abs(inverse_roots)
```
Important observations:

- Because absolute value of the roots > 1, this process is **covariance stationary**.
- Because the roots are complex, the ACF oscillates
- Because the roots are close to 1, the ACF dampens slowly

### Simulate the data and see what it looks like

Now, run the actual simulation with `arima.sim`:
```{r}
x2 <- arima.sim(n=100,list(ar=c(1.5, -0.9), ma=0))
str(x2)
summary(x2)

par(mfrow=c(2,2))
plot(x2, main='Simulated AR(2) process; ar=1.5, ar2=-0.9')
hist(x2, breaks=20, main='Simulated AR(2) process; ar=1.5, ar2=-0.9')
acf(x2, main='Simulated AR(2) process; ar=1.5, ar2=-0.9')
pacf(x2, main='Simulated AR(2) process; ar=1.5, ar2=-0.9')
par(mfrow=c(1,1))
```
Important observations:

- Time series plot shows strong fluctuations, with magnitude changing over time
- Histogram appears fairly symmetric
- ACF oscillates slowly (complex roots; close to 1)
- PACF has sharp cutoff at $p=2$, with second lag term being negative (negative coefficient)

### Pretend we don't know what model it came from; estimate a model

We use the `ar` function with MLE method.  By default, it selects a model by AIC, but that's changeable.

```{r}
(x2_ar <- ar(x2, method='mle'))
x2_ar$order  # order of lowest AIC model
x2_ar$ar  # parameter estimate
x2_ar_se <- sqrt(x2_ar$asy.var)  # calc std error from asymptotic variance
x2_ar$aic  # get the AIC for each order model
```

Note that the model with the lowest AIC will report $AIC=0$, and all the others as a difference from that baseline.  Note the very sharp dropoff in AIC from $p=1$ to $p=2$.

In the async, the best model was estimated with order $p=5$, but parameters and AIC were very close to $p=2$.

### Confidence Intervals on Parameters

We could also calculate a Wald confidence interval for parameters.  Note: example in async is AR(1) which is more straightforward since variance is a scalar, rather than a matrix.
```{r}
(x2_coef1_ci <- x2_ar$ar[1] + c(-1.96, 1.96) * x2_ar_se[1,1])
(x2_coef2_ci <- x2_ar$ar[2] + c(-1.96, 1.96) * x2_ar_se[2,2])
```

### Aside: inconsistent definitions of AIC

AIC can be manually calculated ($k$ = number of params; $T$ = length of sample) as:

$$ AIC = exp(\frac{2k}{T})\frac{\sum_{t=1}^T e_t^2}{T} $$
$$ ln(AIC) = ln(\sum_{t=1}^T e_t^2) + \frac{2k}{T} = ln(MSE) + \frac{2k}{T}$$
Some authors refer to $ln(AIC)$ as simply $AIC$.

Our textbook defines it as:

$$AIC = -2 log(likelihood) + 2k$$

## Model Diagnosis and Testing

1. AR processes have random components resembling white noise.  Do our _estimated_ residuals look like realizations generated by a white noise process?
2. We are interested in stationary AR models.  Is our _estimated_ model stationary?  (If we forgot to remove a trend, R will actually fail to converge and give an error, helping us out!)

### Diagnostics for AR(1) simulation

To explore these, let's do another simulation, AR(1) this time:

$$ x_t - \mu = 0.7(x_{t-1} - \mu) + w_t$$

```{r}
x1 <- arima.sim(n=100, list(ar=c(0.7), ma=0))
str(x1)
summary(x1)

par(mfrow=c(2,2))
plot(x1, main='Simulated AR(1) process; ar=0.7')
hist(x1, breaks=20, main='Simulated AR(1) process; ar=0.7')
acf(x1, main='Simulated AR(1) process; ar=0.7')
pacf(x1, main='Simulated AR(1) process; ar=0.7')
par(mfrow=c(1,1))
```

These all look as expected.

```{r}
(x1_ar <- ar(x1, method='mle'))
x1_ar$order  # order of lowest AIC model
x1_ar$ar  # parameter estimate
(x1_ar_se <- sqrt(x1_ar$asy.var))  # calc std error from asymptotic variance
x1_ar$aic  # get the AIC for each order model
```

Examine the distribution of residuals (to see if they look normal):

```{r}
head(x1_ar$resid)   
par(mfrow=c(1,2))
hist(x1_ar$resid[-1], breaks=20, main='Residuals')  # [-1] tells it to drop the first NA point
qqnorm(x1_ar$resid[-1])  # [-1] tells it to drop the first NA point
par(mfrow=c(1,1))
```
Note there's no value for the first residual, because it's needed to calculate the residual for the second one since this is an AR(1) model.  For AR(2), there would be no value for the first two, etc...

Both the histogram and Q-Q plot show the residuals appear to be pretty normal.

### Diagnostics for AR(2) simulation

First look for normality of the residuals
```{r}
head(x2_ar$resid)   
par(mfrow=c(1,2))
hist(x2_ar$resid[-c(1,2)], breaks=20, main='Residuals')  # [-1] tells it to drop the first NA point
qqnorm(x2_ar$resid[-c(1,2)])  # [-1] tells it to drop the first NA point
par(mfrow=c(1,1))
```

Now plot time series, ACF, and PACF of _residuals_. 

```{r}
# Note that the `fit$resid` object is a time series, but appending `[-c(1,2)]`
# converts it to numeric, so we have to change it back.
x2_resid_ts <- ts(x2_ar$resid[-c(1:5)])
par(mfrow=c(2,2))
plot(x2_resid_ts, main='Residual Time Series')
acf(x2_resid_ts, main='ACF of Residuals')
pacf(x2_resid_ts, main='PACF of Residuals')
par(mfrow=c(1,1))
```

TS looks like white noise.  The ACF drops immediately (which is good), and the PACF doesn't show statistically significant lags (good).

### Order Selection in an AR Model

In the above examples, we chose our model based on AIC.  Alternatively, we could have used BIC, or a test set.  

**IMPORTANT: Unlike in ML, we can't randomly choose points to withhold for a test set, because they'd lose the dependency.  It's common to reserve the final few points of a time series as a test set.**

We should also consider what question we want to answer.  If we are forecasting, is it short term or long term?  If we have infrequent data, we may not want to use a higher order model (e.g. so we don't have to rely on 5 months of data to forecast an AR(5) model)

## Introduction to Moving Average models

A moving average model of order $q$ is a linear combination of the current and past $q$ white noises $w_t$ (aka. "shocks").  Assume that $x_t$ is a demeaned sequence:

$$ x_t = w_t + \beta_1 w_{t-1} + \beta_2 w_{t-2} + ... \beta_{q} w_{t-q}$$

Expressed in backshift operators:

$$ x_t = (1 + \beta_1 B + \beta_2 B^2 + ... + \beta_q B^q) w_t = \phi_q(B)w_t$$

Because MA processes consist of a finite sum of white noise terms, they are stationary with a time-invariant mean, variance, and autocovariance.  In other words, stationarity is met for an MA process, _regardless of its parameters_:

$$ E(x_t) = \sum_{i=0}^q \beta_i E(w_{t-i}) = 0$$
$$ \gamma_0 = Var(x_t) = \sum_{i=0}^q \beta_i^2 Var(w_{t-i}) = (1+\beta_1^2 + ... + \beta_q^2) \sigma_w^2$$
We see that the mean is zero and the variance is a constant.  (Note: we always assume $\beta_0=1$.)  

The autocorrelation function for lag $k \ge 0$, again with $\beta_0=1$:

$$ 
\rho(k) = 
\begin{cases}
  1&,k=0\\
  \big[\sum_{i=0}^{q-k} \beta_i \beta_{i+k} \big]/ \big[ \sum_{i=0}^q \beta_i^2 \big]  &,k=1, ... q\\
  0&,k \ge q
\end{cases}
$$

### Interpreting ACF and PACF plots for MA(q) models

The important takeaway is that the ACF shows that the $MA(q)$ model has a "memory" of only $q$ periods; the current value is _not_ affected by any lag older than $q$ periods.  **We will see the ACF plot drop away suddenly after $q$ periods.**  This happens regardless of the value of the MA $\beta$ parameters.

Also, when we look at the lag $k=1$ line on the ACF plot for MA(1), it will be positive if the $\beta$ parameter is positive and negative if the parameter is negative.

**Unlike the ACF plot, the PACF plot for the MA process will _gradually_ decay to zero.**  This is because of the infinite AR representation of the MA process.  If the $\beta$ parameter is positive, then the PACF will oscillate positive/negative as it decays.  If the parameter is negative, then the PACF will show mostly (but not all) negative spikes.

Higher order MA models have richer dynamics, which can be used to improve forecasting.  However, just based on the plots, it will be hard to distinguish between MA(2) and MA(1) if their first parameters are similar.

Negative $\beta$ parameters in MA models will generate more volatility than positive ones.  Again, the first parameter has a bigger effect than later parameters (due to taking exponents of numbers < 1).

### Invertibility

Now, back to the "shocks"; they are theoretical and _unobservable_.  We cannot use these for forecasting because we don't have an established statistical relationship between current and past values.

The question of whether we can transform a MA model into a form where it can be described as a function of current and past values leads to the concept of _invertibility_.  If the MA process is invertible, it can be transformed into an AR model that's a function of current and past "shocks". This transformation enables forecasting!

** An $MA(q)$ process is invertible when the roots $\phi_q(B)$ all have absolute values > 1.**

_NOTE: I can't follow his derivation of the invertibility condition in 8.8.1._

### MA(1) Simulated Plots

Here are a few examples to look at the effect of different parameters and signs:

```{r}
# MA(1) simulation - effects of positive/negative params
sim1 <- arima.sim(n=100, list(ar=0, ma=c(0.9)))
sim2 <- arima.sim(n=100, list(ar=0, ma=c(0.5)))
sim3 <- arima.sim(n=100, list(ar=0, ma=c(-0.9)))
sim4 <- arima.sim(n=100, list(ar=0, ma=c(-0.5)))
par(mfrow=c(2,2))
plot(sim1, main='MA(1) simulation; c(0.9)')
plot(sim2, main='MA(1) simulation; c(0.5)')
plot(sim3, main='MA(1) simulation; c(-0.9)')
plot(sim4, main='MA(1) simulation; c(-0.5)')
par(mfrow=c(1,1))
```
Interpretation: As described above, negative parameters create more volatility.  Compare the bottom row to the top; also the upper right to upper left. We also note that the magnitude of the parameters (e.g. 0.9 vs. 0.5) doesn't have much effect on the time series.

Next, the ACF plots:
```{r}
# MA(1) simulation - effects of positive/negative params
par(mfrow=c(2,2))
acf(sim1, main='MA(1) simulation; c(0.9)')
acf(sim2, main='MA(1) simulation; c(0.5)')
acf(sim3, main='MA(1) simulation; c(-0.9)')
acf(sim4, main='MA(1) simulation; c(-0.5)')
par(mfrow=c(1,1))
```
Interpretation: We see that they all drop off sharply after the first lag (not the 0th).  Also for lag $k=1$, we see that positive parameters have a positive lag, and vice versa.

Next the PACF plots:
```{r}
# MA(1) simulation - effects of positive/negative params
par(mfrow=c(2,2))
pacf(sim1, main='MA(1) simulation; c(0.9)')
pacf(sim2, main='MA(1) simulation; c(0.5)')
pacf(sim3, main='MA(1) simulation; c(-0.9)')
pacf(sim4, main='MA(1) simulation; c(-0.5)')
par(mfrow=c(1,1))
```
Interpretation: We see that for the positive parameters, the delay oscillates gradually towards zero.  For the negative parameters, the decay is mostly one-sided (negative).

### Comparing MA(1) and MA(2) Simulated Plots

Here are a few examples to look at the effect of different parameters and signs:

```{r}
# MA(1) vs MA(2) simulation - effects of second params
sim1 <- arima.sim(n=100, list(ar=0, ma=c(0.9)))
sim2 <- arima.sim(n=100, list(ar=0, ma=c(0.9, 0.4)))
sim3 <- arima.sim(n=100, list(ar=0, ma=c(0.9, -0.4)))
par(mfrow=c(2,2))
plot(sim1, main='MA(1) simulation; c(0.9)')
plot(sim2, main='MA(2) simulation; c(0.9, 0.4)')
plot(sim3, main='MA(2) simulation; c(0.9, -0.4)')
par(mfrow=c(1,1))
```
Interpretation: The effect of adding a second parameter while keeping the first the same is not easy to distinguish.

### MA(2) Simulated Plots
```{r}
# MA(2) simulation - effects of positive/negative params
sim1 <- arima.sim(n=100, list(ar=0, ma=c(0.9, 0.4)))
sim2 <- arima.sim(n=100, list(ar=0, ma=c(0.9, -0.4)))
sim3 <- arima.sim(n=100, list(ar=0, ma=c(-0.9, 0.4)))
sim4 <- arima.sim(n=100, list(ar=0, ma=c(-0.9, -0.4)))
par(mfrow=c(2,2))
plot(sim1, main='MA(2) simulation; c(0.9, 0.4)')
plot(sim2, main='MA(2) simulation; c(0.9, -0.4)')
plot(sim3, main='MA(2) simulation; c(-0.9, 0.4)')
plot(sim4, main='MA(2) simulation; c(-0.9, -0.4)')
par(mfrow=c(1,1))
```
Interpretation: As described above, negative parameters create more volatility.  Compare the bottom row to the top; also the upper right to upper left.  We also see that the first parameters has more effect than the second.

Now let's look at the ACF plots:
```{r}
par(mfrow=c(2,2))
acf(sim1, main='MA(2) simulation; c(0.9, 0.4)')
acf(sim2, main='MA(2) simulation; c(0.9, -0.4)')
acf(sim3, main='MA(2) simulation; c(-0.9, 0.4)')
acf(sim4, main='MA(2) simulation; c(-0.9, -0.4)')
par(mfrow=c(1,1))
```
Interpretation: TODO
### MA Simulation and Estimation Exercise

Now, run a simulation and then try to estimate it.

```{r}
x3 <- arima.sim(n=1000, list(ar=0, ma=c(0.5, -0.4)))
str(x3)
summary(x3)

par(mfrow=c(2,2))
plot(x3, main='Simulated MA(2) process (0.5, -0.4)')
hist(x3, breaks=20, main='Simulated MA(2) process (0.5, -0.4)')
acf(x3, main='Simulated MA(2) process (0.5, -0.4)')
pacf(x3, main='Simulated MA(2) process (0.5, -0.4)')
par(mfrow=c(1,1))
```

Note that as expected, the ACF plot drops off after lag $k=2$ and the PACF plot gradually decays.  Now let's estimate it using the `arima(p, d, q)` function.  Right now we're only using $q$:

```{r}
(x3_ma <- arima(x3, order=c(0,0,2)))

# take a quick look at the fitted values
summary_df <- data.frame(cbind(x3, fitted(x3_ma), x3_ma$residuals))
head(summary_df)

# concisely display summary statistics with stargazer
stargazer(summary_df, type='text')
```

Now look at diagnostic plots of the residuals:

```{r}
par(mfrow=c(2,2))
plot(x3_ma$residuals, main='Residual Time Series')
hist(x3_ma$residuals, main='Residuals', breaks=20)
acf(x3_ma$residuals, main='ACF of Residuals')
pacf(x3_ma$residuals, main='ACF of Residuals')
par(mfrow=c(1,1))
```

We can also run the **Ljung-Box test** for independence of the residual series.  The test's null hypothesis is of independence.

```{r}
Box.test(x3_ma$residuals, type='Ljung-Box')
```

We cannot reject the null hypothesis of independence.  This is consistent with the ACF and residual time series plots.

Now let's look at the original vs. estimated:

```{r}
# Plot only the last 100 points so that it's easier to see
x3_last100 <- window(x3, start=900, end=1000)
x3_ma_last100 <- window(fitted(x3_ma), start=900, end=1000)
ts.plot(x3_last100, x3_ma_last100,
        gpars = list(col = c("black", "blue"), lty=c(1,2)),
        xlab="Simulated Time Series", ylab='',
        main="Actual vs. Estimated MA(2); params (0.5, -0.4)")
```

Observe that the pointwise fits are not perfect, but we shouldn't expect that; these are only point estimates.  

### Forecasting with MA(q) model

Because of the nature of the $MA(q) model, we can only forecast $q$ steps ahead:

```{r}
forecast(x3_ma,10)
```

We can see that from the $q+1 = 3rd$ step on, the values are constant.